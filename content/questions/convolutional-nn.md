---
filename: convolutional-nn.md
title: Сверточные нейронные сети
Summary: >
  На данный момент сверточная нейронная сеть и ее модификации считаются лучшими по точности и скорости 
  алгоритмами нахождения объектов на сцене
date: 2022-01-10T18:07:05+03:00
билеты: ["Билет 13", ]
draft: false
weight: 19
cover:
  image: https://docs.ecognition.com/Resources/Images/ECogUsr/UG_CNN_scheme.png
author: "Ilya Kochankov"
---

## Сверточные нейронные сети

На данный момент сверточная нейронная сеть и ее модификации считаются лучшими по точности и скорости алгоритмами 
нахождения объектов на сцене. 

Начиная с 2012 года, нейросети занимают первые места на известном международном конкурсе по распознаванию образов ImageNet.

{{< figure src="/images/uploads/convnn-plot.png"
class="figure__image" >}}

## Структура сверточной нейронной сети

СНС состоит из разных видов слоев: 
- сверточные (convolutional) слои
- субдискретизирующие (subsampling, подвыборка) слои 
- слои «обычной» нейронной сети – персептрона

{{< figure src="/images/uploads/conv-example.png"
class="figure__image" >}}

Первые два типа слоев (convolutional, subsampling), чередуясь между собой, формируют входной вектор 
признаков для многослойного персептрона.

Свое название сверточная сеть получила по названию операции – свертка.

На рисунке ниже продемонстрирована визуализация свертки и подвыборки:

{{< figure src="/images/uploads/convolution-example.png"
class="figure__image" >}}

## Свертка

{{< figure src="https://neerc.ifmo.ru/wiki/images/thumb/5/5c/Convolution-operation-on-volume5.png/600px-Convolution-operation-on-volume5.png"
class="figure__image" >}}

Пусть есть пара матриц: \\( A \\) (размера \\( n_x×n_y \\) ) и \\( B \\) (размера \\( m_x×m_y \\) )

Сверткой называется операция, результатом которой является матрица \\( C=A∗B \\) (значок - не умножение!!! это снежинка!!!)

Размер результирующей матрицы:  \\( (n_x−m_x+1)×(n_y−m_y+1) \\)

Каждый элемент \\( i, j \\) результирующей матрицы расчитывается по формуле:

{{< figure src="/images/uploads/convolution-formul.png"
class="figure__image" >}}

## Топология CNN

Определение топологии сети ориентируется на решаемую задачу, данные из научных статей и собственный экспериментальный опыт.

Можно выделить следующие этапы влияющие на выбор топологии:

- определить решаемую задачу нейросетью (классификация, прогнозирование, модификация)
- определить ограничения в решаемой задаче (скорость, точность ответа)
- определить входные (тип: изображение, звук, размер: 100x100, 30x30, формат: RGB, в градациях серого) и выходных данные (количество классов)

{{< figure src="/images/uploads/convolution-topology.png"
class="figure__image" >}}

Входные данные представляют собой цветные картинки. Если размер будет слишком велик, то вычислительная сложность 
повысится, соответственно ограничения на скорость ответа будут нарушены, определение размера в данной задаче решается 
методом подбора. 

Если выбрать размер слишком маленький, то сеть не сможет выявить ключевые признаки.

Входной слой учитывает топологию изображений и состоит из нескольких карт (матриц), 
карта может быть одна, в том случае, если изображение представлено в оттенках серого, 
иначе их 3, где каждая карта соответствует изображению с конкретным каналом (красным, синим и зеленым).

### Сверточный слой

Сверточный слой представляет собой набор карт (другое название – карты признаков, в обиходе это обычные матрицы), 
у каждой карты есть синоптическое ядро (в разных источниках его называют по-разному: сканирующее ядро или фильтр).

Количество карт определяется требованиями к задаче, если взять большое количество карт, то повысится качество 
распознавания, но увеличится вычислительная сложность.

{{< figure src="/images/uploads/conv-layer.png"
class="figure__image" >}}

### Слой подвыборки

Подвыборочный слой также, как и свёрточный имеет карты, но их количество совпадает с предыдущим 
(сверточным) слоем, их 6. 

Цель слоя – уменьшение размерности карт предыдущего слоя. Если на предыдущей операции свертки уже были выявлены 
некоторые признаки, то для дальнейшей обработки настолько подробное изображение уже не нужно, и оно уплотняется 
до менее подробного. 

К тому же фильтрация уже ненужных деталей помогает не переобучаться.

В процессе сканирования ядром подвыборочного слоя (фильтром) карты предыдущего слоя, сканирующее ядро 
не пересекается в отличие от сверточного слоя. 

Обычно, каждая карта имеет ядро размером 2x2, что позволяет уменьшить предыдущие карты сверточного слоя в 2 раза. 
Вся карта признаков разделяется на ячейки 2х2 элемента, из которых выбираются максимальные по значению.

{{< figure src="/images/uploads/maxpooling-layer.png"
class="figure__image" >}}

> Обычно в подвыборочном слое (или прямо перед ним) применяется функция активации RelU. ReLu нелинейна по своей природе, 
> а комбинация ReLu также нелинейна. Добавление RelU добавляет нелинейность модели

### Полносвязный слой

Последний из типов слоев это слой обычного многослойного персептрона. Цель слоя – классификация,
моделирует сложную нелинейную функцию, оптимизируя которую, улучшается качество распознавания.

Выходной слой связан со всеми нейронами предыдущего слоя. Количество нейронов соответствует количеству распознаваемых классов.

## Обучение сверточной нейронной сети

На начальном этапе нейронная сеть является необученной (ненастроенной). 
В общем смысле под обучением понимают последовательное предъявление образа на вход нейросети, 
из обучающего набора, затем полученный ответ сравнивается с желаемым выходом. 

Затем эту дельту ошибки необходимо распространить на все связанные нейроны сети. 

Таким образом обучение нейронной сети сводится к минимизации функции ошибки, путем корректировки весовых коэффициентов 
синаптических связей между нейронами. Под функцией ошибки понимается разность между полученным ответом и желаемым. 

Затем веса выходного слоя нейронов корректируются в соответствии с ошибкой. Для нейронов выходного слоя известны 
их фактические и желаемые значения выходов. Поэтому настройка весов связей для таких нейронов является относительно простой.

Распространение ошибки на полно связные скрытые слоя производится с помощью метода обратного распространения ошибки. 

### Расчет ошибки на подвыборочном слое

Расчет ошибки на подвыборочном слое представляется в нескольких вариантах. 
Первый случай, когда подвыборочный слой находится перед полносвязным, тогда он имеет нейроны и связи такого же типа, 
как в полносвязном слое, соответственно вычисление δ ошибки ничем не отличается от вычисления δ скрытого слоя. 

Второй случай, когда подвыборочный слой находится перед сверточным, вычисление δ происходит путем обратной свертки. 

Для понимания обратно свертки, необходимо сперва понять обычную свертку и то, что скользящее окно по карте признаков 
(во время прямого распространения сигнала) можно интерпретировать, как обычный скрытый слой со связями между нейронами,
но главное отличие — это то, что эти связи разделяемы, то есть одна связь с конкретным значением веса может быть у 
нескольких пар нейронов, а не только одной.

{{< figure src="/images/uploads/maxpooling-backprop.png"
class="figure__image" >}}

Синим цветом обозначена подвыборочная карта, разноцветным – синаптическое ядро, оранжевым – получившаяся свертка

Теперь, когда операция свертки представлена в привычном многослойном виде, можно интуитивно понять, что вычисление дельт происходит таким же образом, 
как и в скрытом слое полносвязной сети. 

Соответственно имея вычисленные ранее дельты сверточного слоя можно вычислить дельты подвыборочного.

{{< figure src="/images/uploads/maxpooling-compute-delta.png"
class="figure__image" >}}

Обратная свертка – это тот же самый способ вычисления дельт, только немного хитрым способом, 
заключающийся в повороте ядра на 180 градусов и скользящем процессе сканирования сверточной 
карты дельт с измененными краевыми эффектами. 

Простыми словами, нам необходимо взять ядро сверточной карты (следующего за подвыборочным слоем) 
повернуть его на 180 градусов и сделать обычную свертку по вычисленным ранее дельтам сверточной карты, 
но так чтобы окно сканирования выходило за пределы карты. 

{{< figure src="/images/uploads/maxpooling-reverce-conv.png"
class="figure__image" >}}

{{< figure src="/images/uploads/maxpoolin-reverce-conv_1.png"
class="figure__image" >}}

### Расчет ошибки на сверточном слое

Обычно впередиидущий слой после сверточного это подвыборочный, соответственно наша задача вычислить дельты текущего 
слоя (сверточного) за счет знаний о дельтах подвыборочного слоя. 

На самом деле дельта ошибка не вычисляется, а копируется. При прямом распространении сигнала нейроны подвыборочного
слоя формировались за счет неперекрывающегося окна сканирования по сверточному слою, в процессе которого выбирались 
нейроны с максимальным значением, при обратном распространении, мы возвращаем дельту ошибки тому ранее выбранному 
максимальному нейрону, остальные же получают нулевую дельту ошибки.

{{< figure src="/images/uploads/maxpooling-conv-backprop.png"
class="figure__image" >}}

{{< figure src="/images/uploads/maxpooling-conv-backprop_1.png"
class="figure__image" >}}

Каждый вес в фильтре вносит свой вклад в каждый пиксель в выходной карте. 
Таким образом, любое изменение веса в фильтре повлияет на все выходные пиксели. 

Все эти изменения складываются, чтобы способствовать окончательной ошибке. Таким образом, мы можем легко вычислить 
производные следующим образом.

{{< figure src="/images/uploads/conv-back-prop.png"
class="figure__image" >}}

Соответственно, что бы вычислить конечную частную производную по корректируемым весам, 
нужно произвести операцию свётрки карты входов, картой ошибки текущего сверточного слоя. 

В результате получится матрица, того же размера что и матрица весов и каждая ее клетка будет соответствовать 
частной производной соответствующей клетке матрицы весов.

## Dropout

Переобучение — одна из проблем глубоких нейронных сетей, состоящая в следующем: модель хорошо объясняет 
только примеры из обучающей выборки, адаптируясь к обучающим примерам, 
вместо того чтобы учиться классифицировать примеры, не участвовавшие в обучении (теряя способность к обобщению). 

За последние годы было предложено множество решений проблемы переобучения, но одно из них превзошло 
все остальные, благодаря своей простоте и прекрасным практическим результатам. это решение — Dropout.

{{< figure src="https://habrastorage.org/r/w1560/web/dd8/171/16f/dd817116fc2348e78272577153e31d2d.jpeg"
class="figure__image" >}}

Термин «dropout» (выбивание, выбрасывание) характеризует исключение определённого процента 
(например 30%) случайных нейронов (находящихся как в скрытых, так и видимых слоях) 
на разных итерациях (эпохах) во время обучения нейронной сети. 

Это очень эффективный способ усреднения моделей внутри нейронной сети. 
В результате более обученные нейроны получают в сети больший вес.

Такой приём значительно увеличивает скорость обучения, качество обучения на тренировочных данных, 
а также повышает качество предсказаний модели на новых тестовых данных.

> Dropout применять не нужно если есть batch-нормализация
