---
filename: learning-params.md
title: Подсчет обучаемых параметров
Summary:
date: 2022-01-10T13:18:40+03:00
билеты: ["Подсчет параметров", ]
draft: false
weight: 17
cover:
  image: https://miro.medium.com/max/2000/1*vkQ0hXDaQv57sALXAJquxA.jpeg
author: "Ilya Kochankov"
---

## Расчет параметров для каждого слоя

Сначала рассмотрим, как рассчитывается количество обучаемых параметров для каждого отдельного типа слоя

### Входной слой

Все, что делает входной слой - это считывает входное изображение, поэтому здесь нет никаких параметров,
которые вы могли бы изучить.

### Свёрточный слой

Рассмотрим свёрточный слой, который принимает l карт объектов на входе и имеет k карт объектов на выходе. 

Размер фильтра - n x m. Например, это будет выглядеть так:

{{< figure src="https://i.stack.imgur.com/2r4XG.png"
class="figure__image" >}}

Здесь входные данные содержат l=32 карты объектов в качестве входных данных, k=64 карты объектов в качестве выходных данных, 
а размер фильтра равен n=3 x m=3. 

Важно понимать, что у нас есть не просто фильтр 3x3, а на самом деле фильтр 3x3x32, 
так как наш вход имеет 32 измерения. И мы изучаем 64 различных фильтра 3x3x32. 

Таким образом, общее количество весов равно n * m * k * l. Затем для каждой карты объектов также существует термин 
смещения, поэтому у нас есть общее количество параметров (n * m * l + 1) * k.

В простейшем случае размер вывода свёрточного слоя равен input_size - (filter_size - 1). 

Это связано с характером свертки: мы используем, например, окрестность 5x5 для вычисления точки, 
но две крайние строки и столбцы не имеют окрестности 5x5, поэтому мы не можем вычислить какие - 
либо выходные данные для этих точек. 

Вот почему наши выходные данные на 2*2=4 строки/столбца меньше, чем входные.

Если вы не хотите, чтобы выходные данные были меньше входных, можно обнулить изображение 
(с параметром pad свёрточного слоя). 

E.g. если вы добавите 2 строки/cols нулей вокруг изображения, размер вывода будет (28+4)-4=28. 
Таким образом, в случае заполнения размер вывода равен input_size + 2*padding - (filter_size -1).

Если вы явно хотите уменьшить размер изображения во время свертки, вы можете определить шаг, например stride=2, 
что означает, что вы перемещаете фильтр с шагом в 2 пикселя. 

Затем выражение становится ((input_size + 2*padding - filter_size)/stride) +1

#### Что такое смещение

Для того чтобы обучение весов, заключенных в кёрнелах, было эффективным, в результаты сверток следует ввести 
некоторое смещение (bias) и нелинейность.

Смещение — это статическая величина, на которую следует “сместить” выходные значения. 
По своей сути это обычная операция сложения каждого элемента выходной матрицы с величиной смещения. 

Если объяснять очень поверхностно, это нужно для того, чтобы вывести нейронную сеть из тупиковых ситуаций, 
имеющих сугубо математические причины.

### Кол-во фильтров в свёрточном слое

Если не задано выходное кол-во слоев, то под k можно еще понимать кол-во фильтров в сверточном слое

### Слой MaxPooling

Слои объединения, например выполните следующие действия: 
"replace a 2x2 neighborhood by its maximum value". 

Таким образом, нет никакого параметра, который вы могли бы изучить в слое объединения

### Полносвязный слой

В полностью подключенном слое все входные блоки имеют отдельный вес для каждого выходного блока. 

Для n входов и m выходов число весов равно n * m. Кроме того, у вас есть смещение для каждого выходного узла, 
поэтому вы находитесь на (n+1) * m параметрах.


## Пример:

{{< figure src="/images/uploads/convolution-params-counting.png"
class="figure__image" >}}
